{
    "gpu": {
        "use": true,
        "id": 0
    },

    
    "model": "GraphiT_perceiver",
    "dataset": "Mof",
    
    "_comment_out_dir": "/scratch/prospero/mselosse/GraphiT_out/GraphiT_perceiver_MOF/",
    "out_dir": "/gpfsstore/rech/tbr/uho58uo/GraphiT_out/GraphiT_perceiver_MOF/",

    "_comment_data_dir": "/scratch/prospero/mselosse/qmof_database/",
    "data_dir": "/gpfsstore/rech/tbr/uho58uo/qmof_database/",
    
    "params": {
        "save_run_tensorboard": true,
        "seed": 35,
        "epochs": 1000,
        "batch_size": 32,
        "init_lr": 7e-4,
        "lr_reduce_factor": 0.5,
        "lr_schedule_patience": 25,
        "min_lr": 1e-10,
        "T_max": 100,
        "weight_decay": 0,
        "print_epoch_interval": 5,
        "warmup": 0,
        "max_time": 96
    },

    "soap_args": {
        "rcut": 8.0,
        "nmax": 6,
        "lmax": 4,
        "sigma": 0.3,
        "pca": 50
    },
    
    "net_params": {
        "double_attention": false,
        "L": 2,
        "L_cross": 3,
        "hidden_dim": 32,
        "out_dim": 32,
        "n_heads": 8,

        "residual": true,
        "readout": "sum",
        "in_feat_dropout": 0.0,
        "dropout": 0.5,
        "feedforward": true,
        "layer_norm": false,
        "batch_norm": true,
        "instance_norm": false,
        
        "use_node_pe": "sum",
        "update_pos_enc": false,
        "node_pe_params": {
            "node_pe": "rand_walk",
            "p_steps": 16
        },

        "use_attention_pe": true,
        "attention_pe_params": {
            "attention_pe": "adj",
            "multi_attention_pe": "per_layer",
            "zero_diag": "false",
            "p_steps": 16,
            "beta": 0.25
        },
        "last_layer_full_attention": false,
        "normalize_degree": true,
        
        "use_edge_features": false,
        "update_edge_features": false,

        "virtual_node": false,
        "cross_dim":16,
        "num_latents":32
    }
}